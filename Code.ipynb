{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca92ad2e-1a8f-4fde-b6c1-e346436541bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "525c322e-b5a5-4125-a7f5-7fcef39db1d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.64</td>\n",
       "      <td>3.10</td>\n",
       "      <td>2.56</td>\n",
       "      <td>15.2</td>\n",
       "      <td>116.0</td>\n",
       "      <td>2.70</td>\n",
       "      <td>3.03</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1.66</td>\n",
       "      <td>5.10</td>\n",
       "      <td>0.96</td>\n",
       "      <td>3.36</td>\n",
       "      <td>845.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.21</td>\n",
       "      <td>4.04</td>\n",
       "      <td>2.44</td>\n",
       "      <td>18.9</td>\n",
       "      <td>111.0</td>\n",
       "      <td>2.85</td>\n",
       "      <td>2.65</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.25</td>\n",
       "      <td>5.24</td>\n",
       "      <td>0.87</td>\n",
       "      <td>3.33</td>\n",
       "      <td>1080.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.93</td>\n",
       "      <td>2.81</td>\n",
       "      <td>2.70</td>\n",
       "      <td>21.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.54</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.75</td>\n",
       "      <td>4.60</td>\n",
       "      <td>0.77</td>\n",
       "      <td>2.31</td>\n",
       "      <td>600.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.73</td>\n",
       "      <td>1.50</td>\n",
       "      <td>2.70</td>\n",
       "      <td>22.5</td>\n",
       "      <td>101.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.29</td>\n",
       "      <td>2.38</td>\n",
       "      <td>5.70</td>\n",
       "      <td>1.19</td>\n",
       "      <td>2.71</td>\n",
       "      <td>1285.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.37</td>\n",
       "      <td>1.17</td>\n",
       "      <td>1.92</td>\n",
       "      <td>19.6</td>\n",
       "      <td>78.0</td>\n",
       "      <td>2.11</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.04</td>\n",
       "      <td>4.68</td>\n",
       "      <td>1.12</td>\n",
       "      <td>3.48</td>\n",
       "      <td>510.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0    13.64        3.10  2.56               15.2      116.0           2.70   \n",
       "1    14.21        4.04  2.44               18.9      111.0           2.85   \n",
       "2    12.93        2.81  2.70               21.0       96.0           1.54   \n",
       "3    13.73        1.50  2.70               22.5      101.0           3.00   \n",
       "4    12.37        1.17  1.92               19.6       78.0           2.11   \n",
       "\n",
       "   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0        3.03                  0.17             1.66             5.10  0.96   \n",
       "1        2.65                  0.30             1.25             5.24  0.87   \n",
       "2        0.50                  0.53             0.75             4.60  0.77   \n",
       "3        3.25                  0.29             2.38             5.70  1.19   \n",
       "4        2.00                  0.27             1.04             4.68  1.12   \n",
       "\n",
       "   od280/od315_of_diluted_wines  proline  label  \n",
       "0                          3.36    845.0      0  \n",
       "1                          3.33   1080.0      0  \n",
       "2                          2.31    600.0      2  \n",
       "3                          2.71   1285.0      0  \n",
       "4                          3.48    510.0      1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine = datasets.load_wine()\n",
    "#wine dataset is consist of wine.data and wine.target \n",
    "#while data are the features(what are the feature of this wine) and target are the label of class(what type of wine)\n",
    "\n",
    "df = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
    "df['label'] = wine.target\n",
    "# Shuffle the DataFrame\n",
    "df = shuffle(df, random_state=42).reset_index(drop=True)\n",
    "# Display the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bcdf9e56-3350-42a1-87e8-063f3d2c067a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = wine.data\n",
    "Y = wine.target\n",
    "#存在xy里面\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "enc = OneHotEncoder()\n",
    "Y = enc.fit_transform(Y[:, np.newaxis]).toarray()\n",
    "#把x的数据normalize\n",
    "#保存了这个数据的分布情况，但是大部分都压缩，利于gd\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "#把数据分为训练数据和测试数据\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=11)\n",
    "\n",
    "X_train = X_train.T \n",
    "X_test = X_test.T \n",
    "Y_train = Y_train.T \n",
    "Y_test = Y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1db52194-457e-4877-8019-fb8d6d744fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gradient(W, X_batch, t_batch):\n",
    "    Y = softmax(W @ X_batch, axis = 0) #this is our model\n",
    "    #     for the prediction\n",
    "    N = X_batch.shape[1] #number of points in batch \n",
    "    K = W.shape[0] #number of classes\n",
    "    M = W.shape[1] #number of features\n",
    "    G = np.zeros((K, M))\n",
    "        #this will store the gradient, the dimensions are K:\n",
    "    #     number of classes, M: number of features\n",
    "        # calculate the gradient according to the formula in the exercises\n",
    "    G = (Y - t_batch) @ X_batch.T / N \n",
    "    return G\n",
    "\n",
    "\n",
    "def logisticGD(X_train, t_train, batch_size, l_rate = 0.1, tol = 1e-5, epochs = 10):\n",
    "    \"\"\"\n",
    "    batch_size = size of training set, correspond to GD batch_size = 1, correspond to SGD\n",
    "    all other values correspond to mini batch GD.\n",
    "    \"\"\"\n",
    "    K = t_train.shape[0]\n",
    "    N = X_train.shape[1]\n",
    "    M = X_train.shape[0]\n",
    "    W = np.random.rand(K, M) #initialization of the model parameters norm_G = float('inf')\n",
    "    norm_G = float('inf')\n",
    "    n_batches = N // batch_size\n",
    "    epoch = 1\n",
    "    while epoch <= epochs and norm_G > tol: \n",
    "        indices = np.random.permutation(N) \n",
    "        X_shuffle = X_train[:, indices] \n",
    "        t_shuffle = t_train[:, indices]\n",
    "        \n",
    "        for j in range(n_batches):\n",
    "            X_batch = X_shuffle[:, j * batch_size:(j + 1) * batch_size] \n",
    "            t_batch = t_shuffle[:, j * batch_size:(j + 1) * batch_size] \n",
    "            G = Gradient(W, X_batch, t_batch)\n",
    "            W = W - l_rate * G\n",
    "            norm_G = np.linalg.norm(W) \n",
    "            epoch += 1\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e5690a5b-adc9-487c-a43e-a4e1dcbbc002",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10000\n",
    "#Training the models with GD, SGD or mini-batch GD\n",
    "#Define the correct value for batch_size\n",
    "### GD\n",
    "batch_size = X_train.shape[1]\n",
    "W_GD = logisticGD(X_train, Y_train, batch_size = batch_size, epochs = epochs, tol = 1e-5, l_rate = 0.001)\n",
    "### SGD\n",
    "batch_size = 1\n",
    "W_SGD = logisticGD(X_train, Y_train, batch_size = batch_size, epochs = epochs, tol = 1e-5, l_rate = 0.001)\n",
    "### Mini batch GD\n",
    "batch_size = X_train.shape[1]//10\n",
    "W_MGD = logisticGD(X_train, Y_train, batch_size = batch_size, epochs = epochs, tol = 1e-5, l_rate = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5689e6dd-bc9c-4f43-87b6-fadfc47e36fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy GD: 0.9699248120300752\n",
      "Test Accuracy GD: 0.9555555555555556\n",
      "Training Accuracy SGD: 0.9774436090225563\n",
      "Test Accuracy SGD: 0.9777777777777777\n",
      "Training Accuracy mini batch GD: 0.9849624060150376\n",
      "Test Accuracy mini batch GD: 0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy on the training set\n",
    "Y_train_pred = softmax(W_GD @ (X_train)) \n",
    "train_predictions = np.argmax(Y_train_pred, axis=0) \n",
    "train_true = np.argmax(Y_train, axis = 0)\n",
    "train_accuracy = np.mean(train_predictions == train_true) \n",
    "print(\"Training Accuracy GD:\", train_accuracy)\n",
    "# Calculate accuracy on the test set\n",
    "Y_test_pred = softmax(W_GD @ (X_test)) \n",
    "test_predictions = np.argmax(Y_test_pred, axis=0) \n",
    "test_true = np.argmax(Y_test, axis = 0)\n",
    "test_accuracy = np.mean(test_predictions == test_true) \n",
    "print(\"Test Accuracy GD:\", test_accuracy)\n",
    "# Calculate accuracy on the training set\n",
    "Y_train_pred = softmax(W_SGD @ (X_train)) \n",
    "train_predictions = np.argmax(Y_train_pred, axis=0) \n",
    "train_true = np.argmax(Y_train, axis = 0)\n",
    "train_accuracy = np.mean(train_predictions == train_true) \n",
    "print(\"Training Accuracy SGD:\", train_accuracy)\n",
    "# Calculate accuracy on the test set\n",
    "Y_test_pred = softmax(W_SGD @ (X_test)) \n",
    "test_predictions = np.argmax(Y_test_pred, axis=0) \n",
    "test_true = np.argmax(Y_test, axis = 0)\n",
    "test_accuracy = np.mean(test_predictions == test_true) \n",
    "print(\"Test Accuracy SGD:\", test_accuracy)\n",
    "# Calculate accuracy on the training set\n",
    "Y_train_pred = softmax(W_MGD @ (X_train)) \n",
    "train_predictions = np.argmax(Y_train_pred, axis=0) \n",
    "train_true = np.argmax(Y_train, axis = 0)\n",
    "train_accuracy = np.mean(train_predictions == train_true) \n",
    "print(\"Training Accuracy mini batch GD:\", train_accuracy)\n",
    "# Calculate accuracy on the test set\n",
    "Y_test_pred = softmax(W_MGD @ (X_test)) \n",
    "test_predictions = np.argmax(Y_test_pred, axis=0) \n",
    "test_true = np.argmax(Y_test, axis = 0)\n",
    "test_accuracy = np.mean(test_predictions == test_true) \n",
    "print(\"Test Accuracy mini batch GD:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cc7747-d949-4fe1-ad23-53f8c430134b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
